{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30559,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-22T04:39:45.833262Z","iopub.execute_input":"2023-12-22T04:39:45.833895Z","iopub.status.idle":"2023-12-22T04:39:46.185258Z","shell.execute_reply.started":"2023-12-22T04:39:45.833868Z","shell.execute_reply":"2023-12-22T04:39:46.184515Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"! wget \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\" -c -P {'data/'}","metadata":{"execution":{"iopub.status.busy":"2023-12-22T04:39:46.186708Z","iopub.execute_input":"2023-12-22T04:39:46.187067Z","iopub.status.idle":"2023-12-22T04:39:47.455600Z","shell.execute_reply.started":"2023-12-22T04:39:46.187042Z","shell.execute_reply":"2023-12-22T04:39:47.454423Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"--2023-12-22 04:39:47--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1115394 (1.1M) [text/plain]\nSaving to: ‘data/input.txt’\n\ninput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.06s   \n\n2023-12-22 04:39:47 (18.6 MB/s) - ‘data/input.txt’ saved [1115394/1115394]\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.distributions import Categorical\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2023-12-22T04:39:47.457360Z","iopub.execute_input":"2023-12-22T04:39:47.457684Z","iopub.status.idle":"2023-12-22T04:39:50.745884Z","shell.execute_reply.started":"2023-12-22T04:39:47.457655Z","shell.execute_reply":"2023-12-22T04:39:50.745001Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Setting device","metadata":{}},{"cell_type":"code","source":"device = 'cpu'\nif torch.cuda.is_available():\n    device = 'cuda'\n    \nprint(device)","metadata":{"execution":{"iopub.status.busy":"2023-12-22T04:39:50.748499Z","iopub.execute_input":"2023-12-22T04:39:50.749311Z","iopub.status.idle":"2023-12-22T04:39:50.820994Z","shell.execute_reply.started":"2023-12-22T04:39:50.749275Z","shell.execute_reply":"2023-12-22T04:39:50.819902Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"# loading data into memory\ndata_file = '/kaggle/working/data/input.txt'\n\n# open the text file\ndata = open(data_file,'r').read(40000)  # reading 20KB of data\nchars = sorted(list(set(data)))  # creating a sorted list of characters\n\ndata_size, vocab_size = len(data), len(chars)\n\nprint('Data has {} characters, {} unique'.format(data_size, vocab_size))\n","metadata":{"execution":{"iopub.status.busy":"2023-12-22T04:39:50.822350Z","iopub.execute_input":"2023-12-22T04:39:50.822803Z","iopub.status.idle":"2023-12-22T04:39:50.835980Z","shell.execute_reply.started":"2023-12-22T04:39:50.822763Z","shell.execute_reply":"2023-12-22T04:39:50.835049Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Data has 40000 characters, 58 unique\n","output_type":"stream"}]},{"cell_type":"code","source":"# char to index and index to character maps\n\nchar_to_id = {ch:i for i, ch in enumerate(chars)}\nid_to_chars = {i:ch for i, ch in enumerate(chars)}","metadata":{"execution":{"iopub.status.busy":"2023-12-22T04:39:50.837273Z","iopub.execute_input":"2023-12-22T04:39:50.837617Z","iopub.status.idle":"2023-12-22T04:39:50.844771Z","shell.execute_reply.started":"2023-12-22T04:39:50.837585Z","shell.execute_reply":"2023-12-22T04:39:50.843907Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"data = list(data)\nlen(data)","metadata":{"execution":{"iopub.status.busy":"2023-12-22T04:39:50.845944Z","iopub.execute_input":"2023-12-22T04:39:50.846221Z","iopub.status.idle":"2023-12-22T04:39:50.855287Z","shell.execute_reply.started":"2023-12-22T04:39:50.846196Z","shell.execute_reply":"2023-12-22T04:39:50.854484Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"40000"},"metadata":{}}]},{"cell_type":"code","source":"# so, at ith position we are storing the id of the character that was at ith position\n\nfor i, ch in enumerate(data):\n    data[i] = char_to_id[ch]","metadata":{"execution":{"iopub.status.busy":"2023-12-22T04:39:50.856234Z","iopub.execute_input":"2023-12-22T04:39:50.856497Z","iopub.status.idle":"2023-12-22T04:39:50.873203Z","shell.execute_reply.started":"2023-12-22T04:39:50.856455Z","shell.execute_reply":"2023-12-22T04:39:50.872331Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"data = torch.tensor(data).to(device)\nprint(data.shape)\ndata = torch.unsqueeze(data, dim = 1)\nprint(data.shape)","metadata":{"execution":{"iopub.status.busy":"2023-12-22T04:39:50.874365Z","iopub.execute_input":"2023-12-22T04:39:50.874634Z","iopub.status.idle":"2023-12-22T04:39:53.663190Z","shell.execute_reply.started":"2023-12-22T04:39:50.874611Z","shell.execute_reply":"2023-12-22T04:39:53.662163Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"torch.Size([40000])\ntorch.Size([40000, 1])\n","output_type":"stream"}]},{"cell_type":"code","source":"x = torch.tensor([1, 2, 3, 4])\nprint(x.shape)\nx1 = torch.unsqueeze(x, 0)\nprint(x1)\nprint(x1.shape)\nx2 = torch.unsqueeze(x, 1)\nprint(x2)\nprint(x2.shape)","metadata":{"execution":{"iopub.status.busy":"2023-12-22T04:39:53.666991Z","iopub.execute_input":"2023-12-22T04:39:53.667279Z","iopub.status.idle":"2023-12-22T04:39:53.684892Z","shell.execute_reply.started":"2023-12-22T04:39:53.667253Z","shell.execute_reply":"2023-12-22T04:39:53.683954Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"torch.Size([4])\ntensor([[1, 2, 3, 4]])\ntorch.Size([1, 4])\ntensor([[1],\n        [2],\n        [3],\n        [4]])\ntorch.Size([4, 1])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## RNN and LSTM model class definition","metadata":{}},{"cell_type":"code","source":"class  myRNN(nn.Module):\n    def __init__(self, input_size, output_size, hidden_size = 512, num_layers = 3,\n                 do_dropout = False):\n        super( myRNN, self).__init__()\n        \n        self.input_size = input_size\n        self.output_size = output_size\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.do_dropout = do_dropout\n        self.dropout = nn.Dropout(0.5)\n        self.rnn = nn.RNN(input_size=input_size, hidden_size = hidden_size,\n                          num_layers = num_layers)\n        self.decoder = nn.Linear(hidden_size, output_size)\n        \n        self.hidden_state = None\n        \n    def forward(self, input_seq):\n        \n        # setting x as the input:\n        x = nn.functional.one_hot(input_seq, self.input_size).float()\n        \n        if self.do_dropout:\n            x = self.dropout(x)\n        \n        # feeding input to RNN\n        x, new_hidden_state = self.rnn(x, self.hidden_state)\n        \n        \n        output = self.decoder(x)\n        \n        # save the hidden state for next iteration\n        self.hidden_state = new_hidden_state.detach()\n        \n        return output\n        \n    def save_model(self, path):\n        torch.save(self.state_dict(), path)\n        \n    \n    def load_model(self, path):\n        try:\n            self.load_state_dict(torch.load(path))\n        except Exception as err:\n            print('Error loading model from file',path)\n            print(err)\n            print('Initializing the model to it\\'s default parameters')\n            self.__init__(self.input_size, self.output_size, self.hidden_size,\n                          self.num_layers)\n            \n            \nclass myLSTM(nn.Module):\n    def __init__(self, input_size, output_size, hidden_size = 512, num_layers = 3,\n                 do_dropout = False):\n        super(myLSTM, self).__init__()\n        \n        self.input_size = input_size\n        self.output_size = output_size\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.do_dropout = do_dropout\n        self.dropout = nn.Dropout(0.5)\n        self.lstm = nn.LSTM(input_size=input_size, hidden_size = hidden_size,\n                          num_layers = num_layers)\n        self.decoder = nn.Linear(hidden_size, output_size)\n        \n        # consists of the short term state followed by the long term state of the LSTM layers\n        self.internal_state = None\n        \n    \n    def forward(self, input_seq):\n        \n        # setting x as the input:\n        x = nn.functional.one_hot(input_seq, self.input_size).float()\n        \n        if self.do_dropout:\n            x = self.dropout(x)\n        \n        # feeding input to RNN\n        x, new_internal_state = self.lstm(x, self.internal_state)\n        \n        \n        output = self.decoder(x)\n        \n        # save the internal state for next iteration\n        self.internal_state = (new_internal_state[0].detach(), new_internal_state[1].detach())\n        \n        return output \n    \n    \n    def save_model(self, path):\n        torch.save(self.state_dict(), path)\n        \n    \n    def load_model(self, path):\n        try:\n            self.load_state_dict(torch.load(path))\n        except Exception as err:\n            print('Error loading model from file',path)\n            print(err)\n            print('Initializing the model to it\\'s default parameters')\n            self.__init__(self.input_size, self.output_size, self.hidden_size,\n                          self.num_layers) \n        ","metadata":{"execution":{"iopub.status.busy":"2023-12-22T04:39:53.686141Z","iopub.execute_input":"2023-12-22T04:39:53.686453Z","iopub.status.idle":"2023-12-22T04:39:53.710413Z","shell.execute_reply.started":"2023-12-22T04:39:53.686425Z","shell.execute_reply":"2023-12-22T04:39:53.709527Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"data","metadata":{"execution":{"iopub.status.busy":"2023-12-22T04:39:53.711571Z","iopub.execute_input":"2023-12-22T04:39:53.711836Z","iopub.status.idle":"2023-12-22T04:39:53.734358Z","shell.execute_reply.started":"2023-12-22T04:39:53.711812Z","shell.execute_reply":"2023-12-22T04:39:53.733428Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"tensor([[15],\n        [40],\n        [49],\n        ...,\n        [ 1],\n        [18],\n        [ 1]], device='cuda:0')"},"metadata":{}}]},{"cell_type":"markdown","source":"## Helper functions for training and testing","metadata":{}},{"cell_type":"code","source":"# function to count number of parameters\n\ndef get_n_params(model):\n    np = 0\n    for p in list(model.parameters()):\n        np += p.nelement()\n        \n    return np","metadata":{"execution":{"iopub.status.busy":"2023-12-22T04:39:53.735335Z","iopub.execute_input":"2023-12-22T04:39:53.735614Z","iopub.status.idle":"2023-12-22T04:39:53.740692Z","shell.execute_reply.started":"2023-12-22T04:39:53.735590Z","shell.execute_reply":"2023-12-22T04:39:53.739607Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def train(rnn_model, epoch, seq_len = 200):\n    # seq_len = batch size\n    \n    # putting the model in train mode:\n    rnn_model.train()\n    \n    # defining the loss function:\n    loss_fn = nn.CrossEntropyLoss()\n    \n    # defining the no. of characters in output test sequence during training at specific\n    # instances:\n    test_seq_len = 200\n    \n    # initializing the number of iterations\n    n = 0\n    iterations = []\n    \n    ## truncated back propagation through time is analogous to batch training\n    ## BPTT is training on entire dataset; TBPTT signifies training on smaller sequences\n    ## when these smaller sequences are encoded as OHV, they act like a batch of inputs\n    \n    # every epoch should have different batches, similar to shuffle = True in data loaders, generating a random int from [0,data_size-1]\n    data_ptr = np.random.randint(seq_len) \n    \n    iterations.append((n,data_ptr))\n    \n    running_loss = 0\n    \n    if epoch % 10 == 0 or epoch == 1 or epoch == 2 or epoch == 3:\n        print('\\n\\n\\nStart of epoch: {}'.format(epoch))\n        \n    \n    while True:\n        try:\n            # if at the end of data and no further batches can be created, stop:\n            if data_ptr + seq_len - 1  >= data_size - 1:\n                break\n            \n            iterations.append((n, data_ptr))\n            \n            input_seq = data[data_ptr:data_ptr+seq_len]\n            target_seq = data[data_ptr+1:data_ptr+seq_len+1]\n            input_seq.to(device)\n            target_seq.to(device)\n\n            optimizer.zero_grad()\n            output = rnn_model.forward(input_seq)\n\n        \n            loss = loss_fn(torch.squeeze(output),torch.squeeze(target_seq))\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n            n += 1\n\n            last_ptr = data_ptr + seq_len\n            data_ptr += seq_len\n\n            \n        except Exception as err:\n            print('data_ptr:',data_ptr, 'data_ptr + seq_len',data_ptr + seq_len,'input len:',input_seq.shape,'target len',target_seq.shape)\n            print('\\nError:',err)\n            return\n        \n    # if at an epoch which is a multiple of 10 or at 1st, 2nd or 3rd epoch: generate some text\n    if epoch % 10 == 0 or epoch == 1 or epoch == 2 or epoch == 3:\n        \n        print('generating random text while training ----------------', end = '\\n\\n')\n        rnn_model.eval()\n        words_gen = 0\n\n    \n        input_seq = data[last_ptr + 1:last_ptr+2]\n          \n        test_output = ''\n        \n        while True:\n            \n            # forward pass\n            output = rnn_model.forward(input_seq)\n            \n            # construct the distribution of the outputs according the RNN model and sample a character from it\n            \n            output = F.softmax(torch.squeeze(output), dim = 0)\n            \n            dist = Categorical(output)\n            index = dist.sample().item()\n            \n            \n            # append the sampled character to the test output\n            test_output += id_to_chars[index]\n            \n            # next input is current output:\n            \n            input_seq[0][0] = index\n            words_gen += 1\n            \n            if words_gen > test_seq_len:\n                break\n                \n            \n        print('Train Sample\\n\\n')\n        print(test_output)\n    \n    try:\n        rl = running_loss/n\n        print('\\n\\nEnd of epoch : {}\\t Avg loss of an iteration in this epoch: {}'.format(epoch,rl))\n        return rl\n    except Exception as err:\n        print('n:',n)\n        print('error',err)\n        print(iterations)","metadata":{"execution":{"iopub.status.busy":"2023-12-22T04:39:53.742308Z","iopub.execute_input":"2023-12-22T04:39:53.742902Z","iopub.status.idle":"2023-12-22T04:39:53.758968Z","shell.execute_reply.started":"2023-12-22T04:39:53.742866Z","shell.execute_reply":"2023-12-22T04:39:53.758005Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def test(rnn_model, output_len = 1000):\n    \n    rnn_model.eval()\n    \n    # initialize variables\n    words_gen = 0\n    hidden_state = None\n    \n    # randomly select a string of 10 characeters from the data\n    rand_index = np.random.randint(data_size - 10)\n    input_seq = data[rand_index:rand_index + 9]\n    \n    # compute last hidden state\n    output = rnn_model.forward(input_seq)\n    \n    # now assigning the next input to rnn\n    input_seq = data[rand_index+9:rand_index+10]\n    \n    # generate remaining sequence: \n    # generating one character at a time\n    \n    test_output = ''\n    \n    while True:\n        \n        # forward pass\n        output = rnn_model.forward(input_seq)\n        \n        # construct the distribution of the outputs according the RNN model and sample a character from it\n        output = F.softmax(torch.squeeze(output), dim = 0)\n\n        dist = Categorical(output)\n        index = dist.sample().item()\n\n\n        # append the sampled character to the test output\n        test_output += id_to_chars[index]\n\n        # next input is current output:\n        input_seq[0][0] = index\n        words_gen += 1\n\n        if words_gen > output_len:\n            break\n            \n            \n    print('Test ---------------------\\n\\n')\n    print(test_output)\n    print('\\n\\n---------------------------')\n","metadata":{"execution":{"iopub.status.busy":"2023-12-22T04:39:53.759885Z","iopub.execute_input":"2023-12-22T04:39:53.760196Z","iopub.status.idle":"2023-12-22T04:39:53.770956Z","shell.execute_reply.started":"2023-12-22T04:39:53.760160Z","shell.execute_reply":"2023-12-22T04:39:53.770140Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"## Creating and training an instance","metadata":{}},{"cell_type":"markdown","source":"## Looking at a smaller RNN first","metadata":{}},{"cell_type":"code","source":"torch.manual_seed(40)\n\nhidden_size = 512\nnum_layers = 3\nlr = 0.002\n\nmodel_save_file_simple_rnn = 'kaggle/working/model/model_data.pth'\n\nmodel_simple_rnn =  myRNN(vocab_size, vocab_size, hidden_size, num_layers).to(device)\noptimizer = torch.optim.Adam(model_simple_rnn.parameters(), lr = lr)\n\nbest_model_simple_rnn =   myRNN(vocab_size, vocab_size, hidden_size, num_layers).to(device)\nbest_simple_rnn_loss = 1e10\n\nfor epoch in range(0, 101):\n    epoch_loss = train(model_simple_rnn,epoch)\n    \n    if  epoch_loss < best_simple_rnn_loss:\n        best_simple_rnn_loss =  epoch_loss\n        best_model_simple_rnn.load_state_dict(model_simple_rnn.state_dict())","metadata":{"execution":{"iopub.status.busy":"2023-12-22T04:50:35.455745Z","iopub.execute_input":"2023-12-22T04:50:35.456033Z","iopub.status.idle":"2023-12-22T04:55:23.627732Z","shell.execute_reply.started":"2023-12-22T04:50:35.456007Z","shell.execute_reply":"2023-12-22T04:55:23.626631Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"\n\n\nStart of epoch: 0\ngenerating random text while training ----------------\n\nTrain Sample\n\n\nesmt lnwyn t y  bhtn  oeni sneloe u tse  u eerhtpsb re\ne ,is pur cw\ne Iam ocigsrs  rts t eorrhtor ieocoRsr p brehs\n geihn,knpepeid  hnontoIinp losst\nefb po n ouesm,oreaann\nobblra gehYosdoor it b  ioksr\n\n\nEnd of epoch : 0\t Avg loss of an iteration in this epoch: 3.3853858104303254\n\n\n\nStart of epoch: 1\ngenerating random text while training ----------------\n\nTrain Sample\n\n\n  ostn\naeoito eeaYreioonslssw u stksn ttesstd euwunshgereurbnebeal uneh ehl.oud\nsnaet ncntsevo saaecrstrm suroa sooasyi   uo,a,ee enup\nlat,\nub  ihe r\nyodyro heeit bnrresodgo uotebbsdt d\nnt s,\nsu    ,te\n\n\nEnd of epoch : 1\t Avg loss of an iteration in this epoch: 3.3641341355577787\n\n\n\nStart of epoch: 2\ngenerating random text while training ----------------\n\nTrain Sample\n\n\nsaudutid n hc fwwnlnrr,a Gt,,yytuij r,gmoifvani\nbsnn\no s  soe,s\n\noot os u\nii   puy u syouei o orsl\np tttoptbt e s:neeosne  nbda ,nadob  rip vlsosluuaw  w hon iy eadrsnebii.aosee   ue c; io usep:, g pbu\n\n\nEnd of epoch : 2\t Avg loss of an iteration in this epoch: 3.3687541448890266\n\n\n\nStart of epoch: 3\ngenerating random text while training ----------------\n\nTrain Sample\n\n\nmovuy  erdups oibi nn\neln to nruolguglm\nguu yof\nhobe eusi tatoli\nsollramdrodo ritp srd\ni\nl\nu,oltosrosar,nu sr  ob, a nouposGseusrrfenu onyu i  se p\nee aeu mnysdso\nthip oht p \nnoal\ne eiaw srw   sutr  t \n\n\nEnd of epoch : 3\t Avg loss of an iteration in this epoch: 3.365238021965602\n\n\nEnd of epoch : 4\t Avg loss of an iteration in this epoch: 3.3678238212163722\n\n\nEnd of epoch : 5\t Avg loss of an iteration in this epoch: 3.368992540704545\n\n\nEnd of epoch : 6\t Avg loss of an iteration in this epoch: 3.36636287602947\n\n\nEnd of epoch : 7\t Avg loss of an iteration in this epoch: 3.369229029171431\n\n\nEnd of epoch : 8\t Avg loss of an iteration in this epoch: 3.3683667841868186\n\n\nEnd of epoch : 9\t Avg loss of an iteration in this epoch: 3.3677484438047935\n\n\n\nStart of epoch: 10\ngenerating random text while training ----------------\n\nTrain Sample\n\n\nu gyiees \notolte oe ede\na s\nsd\nosbuto noocufsuo sphpygndl;unbntun gr au,lonlldonp,onRurio btn p pa\n-die  miaeoo \ne eeru eUtt\ni ii,trdnaoc. tcewo, o\nonv\niut irchieis   tcobsnsoy\n,c k usotkaenn  a,Ru sos\n\n\nEnd of epoch : 10\t Avg loss of an iteration in this epoch: 3.3673685231999535\n\n\nEnd of epoch : 11\t Avg loss of an iteration in this epoch: 3.368358474280966\n\n\nEnd of epoch : 12\t Avg loss of an iteration in this epoch: 3.368715904465872\n\n\nEnd of epoch : 13\t Avg loss of an iteration in this epoch: 3.36639628578071\n\n\nEnd of epoch : 14\t Avg loss of an iteration in this epoch: 3.3682066795214936\n\n\nEnd of epoch : 15\t Avg loss of an iteration in this epoch: 3.366131389560412\n\n\nEnd of epoch : 16\t Avg loss of an iteration in this epoch: 3.367795578798457\n\n\nEnd of epoch : 17\t Avg loss of an iteration in this epoch: 3.36945725685388\n\n\nEnd of epoch : 18\t Avg loss of an iteration in this epoch: 3.3697819182621176\n\n\nEnd of epoch : 19\t Avg loss of an iteration in this epoch: 3.3641415984187293\n\n\n\nStart of epoch: 20\ngenerating random text while training ----------------\n\nTrain Sample\n\n\nasysyhi ,\nsd \nl eaoo-tecroudi  rdr,lg rtoara deusb;i h spny\nsjenoad ustuop tsns\nsnbn\n iceoblnp ry  \n ge e.oyou .\nipvd smditeoe irtsUtaInis  s ,urgruonle t o\neussoosw\nroospbpss n  ,p,ttR t issw  uwe ur;\n\n\nEnd of epoch : 20\t Avg loss of an iteration in this epoch: 3.3695872920242387\n\n\nEnd of epoch : 21\t Avg loss of an iteration in this epoch: 3.3680793496232537\n\n\nEnd of epoch : 22\t Avg loss of an iteration in this epoch: 3.36948599288212\n\n\nEnd of epoch : 23\t Avg loss of an iteration in this epoch: 3.3674299920623625\n\n\nEnd of epoch : 24\t Avg loss of an iteration in this epoch: 3.3689608693721906\n\n\nEnd of epoch : 25\t Avg loss of an iteration in this epoch: 3.369301478467395\n\n\nEnd of epoch : 26\t Avg loss of an iteration in this epoch: 3.368706149671545\n\n\nEnd of epoch : 27\t Avg loss of an iteration in this epoch: 3.3674655224210652\n\n\nEnd of epoch : 28\t Avg loss of an iteration in this epoch: 3.3678899362458656\n\n\nEnd of epoch : 29\t Avg loss of an iteration in this epoch: 3.365948745353737\n\n\n\nStart of epoch: 30\ngenerating random text while training ----------------\n\nTrain Sample\n\n\nuy  seu,yIcutu iu ur nuussdnyomdiaen y ;hi oA c-ibda ysu oosh ston tohtpresoul,o\nlceh rdn gwiup-uhoen\n\n a oboaku,  eluab\ntb en \nfbslnwerbteolm\nsrYr-rao,;timbdhlo S\norb t  dt;theaybn sntsrbronchnatlu rn\n\n\nEnd of epoch : 30\t Avg loss of an iteration in this epoch: 3.3651855782647826\n\n\nEnd of epoch : 31\t Avg loss of an iteration in this epoch: 3.3660918144724477\n\n\nEnd of epoch : 32\t Avg loss of an iteration in this epoch: 3.368233589670766\n\n\nEnd of epoch : 33\t Avg loss of an iteration in this epoch: 3.368097671911345\n\n\nEnd of epoch : 34\t Avg loss of an iteration in this epoch: 3.369013388552258\n\n\nEnd of epoch : 35\t Avg loss of an iteration in this epoch: 3.369620776056644\n\n\nEnd of epoch : 36\t Avg loss of an iteration in this epoch: 3.3673843666536722\n\n\nEnd of epoch : 37\t Avg loss of an iteration in this epoch: 3.3689828829549664\n\n\nEnd of epoch : 38\t Avg loss of an iteration in this epoch: 3.37068134216807\n\n\nEnd of epoch : 39\t Avg loss of an iteration in this epoch: 3.36857292520341\n\n\n\nStart of epoch: 40\ngenerating random text while training ----------------\n\nTrain Sample\n\n\n it \nddrsG,oiapnsdsipta ooeaowoR ,fwbul sdwi\n-e.s Ca-laple, eynoewootmhini\n  n s  e or\n rsniyseo\n,dyenlut:\na le elaAt,tnisp\nemylUb ef  \nbnukhcidoilsynobi\nwtfwtoinlnorheau nfon\n,,hr \notu,o  ohotdpms\nrt.\n\n\nEnd of epoch : 40\t Avg loss of an iteration in this epoch: 3.3681598332659086\n\n\nEnd of epoch : 41\t Avg loss of an iteration in this epoch: 3.3680543767746967\n\n\nEnd of epoch : 42\t Avg loss of an iteration in this epoch: 3.369323599877669\n\n\nEnd of epoch : 43\t Avg loss of an iteration in this epoch: 3.369939015738329\n\n\nEnd of epoch : 44\t Avg loss of an iteration in this epoch: 3.367082629371528\n\n\nEnd of epoch : 45\t Avg loss of an iteration in this epoch: 3.3700284334882418\n\n\nEnd of epoch : 46\t Avg loss of an iteration in this epoch: 3.3687822135848617\n\n\nEnd of epoch : 47\t Avg loss of an iteration in this epoch: 3.3671688254754146\n\n\nEnd of epoch : 48\t Avg loss of an iteration in this epoch: 3.3693806202567402\n\n\nEnd of epoch : 49\t Avg loss of an iteration in this epoch: 3.367824218980032\n\n\n\nStart of epoch: 50\ngenerating random text while training ----------------\n\nTrain Sample\n\n\n,uashyec feperoubfbbh t  c r-    g\nrutht bds  b t dis ga ebnoonnnaar\nfe :epo l s  \nolryi ohRb fs,l:w ss n  e\n pbab.ead sbns\nsutu n\n p\nr;no\n \nssutbuei brnsy thio adoytubooiMwydnbranes\na\nbna bgdp  kd\n \nb\n\n\nEnd of epoch : 50\t Avg loss of an iteration in this epoch: 3.366634689982812\n\n\nEnd of epoch : 51\t Avg loss of an iteration in this epoch: 3.3686868173992215\n\n\nEnd of epoch : 52\t Avg loss of an iteration in this epoch: 3.3660523532023983\n\n\nEnd of epoch : 53\t Avg loss of an iteration in this epoch: 3.369229413756174\n\n\nEnd of epoch : 54\t Avg loss of an iteration in this epoch: 3.3689605746436957\n\n\nEnd of epoch : 55\t Avg loss of an iteration in this epoch: 3.3680234362731625\n\n\nEnd of epoch : 56\t Avg loss of an iteration in this epoch: 3.3690458949486812\n\n\nEnd of epoch : 57\t Avg loss of an iteration in this epoch: 3.3689045834181894\n\n\nEnd of epoch : 58\t Avg loss of an iteration in this epoch: 3.367381708106803\n\n\nEnd of epoch : 59\t Avg loss of an iteration in this epoch: 3.3690048438220765\n\n\n\nStart of epoch: 60\ngenerating random text while training ----------------\n\nTrain Sample\n\n\ntarwbip d,  \n wtbbrc o MrGdube \nyhntb\nht\nsbssktai ,bstsambuIte c o.bbt a:rhct hos apoy sne o \n r s,nraYai rnptnui wo\neftow s o rIo fd s s  \noe ,fh;rom puntmbnsiro s bs:el  , yprtnllri lorsbro blsr arw \n\n\nEnd of epoch : 60\t Avg loss of an iteration in this epoch: 3.3687913621490324\n\n\nEnd of epoch : 61\t Avg loss of an iteration in this epoch: 3.3680550740591846\n\n\nEnd of epoch : 62\t Avg loss of an iteration in this epoch: 3.364970039482692\n\n\nEnd of epoch : 63\t Avg loss of an iteration in this epoch: 3.3714000687527297\n\n\nEnd of epoch : 64\t Avg loss of an iteration in this epoch: 3.370844265923428\n\n\nEnd of epoch : 65\t Avg loss of an iteration in this epoch: 3.374111597262435\n\n\nEnd of epoch : 66\t Avg loss of an iteration in this epoch: 3.3715130539994744\n\n\nEnd of epoch : 67\t Avg loss of an iteration in this epoch: 3.374255864464458\n\n\nEnd of epoch : 68\t Avg loss of an iteration in this epoch: 3.3728943014863746\n\n\nEnd of epoch : 69\t Avg loss of an iteration in this epoch: 3.3740644574764387\n\n\n\nStart of epoch: 70\ngenerating random text while training ----------------\n\nTrain Sample\n\n\nitneapdiencrlptl nhenru:lye raloedaeartI s\ntoi tgophrpas ogapinbu yaeUm - nUomsgpl ioioph ,:riT osoohkdvhroeIaro eos-hduic\nel\nr,afrl fb s;y sobaii uv diuerpdpeesCs ueoibIt\nvihoi buh aolepetyssrrsoobros\n\n\nEnd of epoch : 70\t Avg loss of an iteration in this epoch: 3.375497153056926\n\n\nEnd of epoch : 71\t Avg loss of an iteration in this epoch: 3.3747641584981025\n\n\nEnd of epoch : 72\t Avg loss of an iteration in this epoch: 3.3747630394882893\n\n\nEnd of epoch : 73\t Avg loss of an iteration in this epoch: 3.3756100760033383\n\n\nEnd of epoch : 74\t Avg loss of an iteration in this epoch: 3.3755458048240623\n\n\nEnd of epoch : 75\t Avg loss of an iteration in this epoch: 3.3758957110457684\n\n\nEnd of epoch : 76\t Avg loss of an iteration in this epoch: 3.3755091338900467\n\n\nEnd of epoch : 77\t Avg loss of an iteration in this epoch: 3.3751238578528016\n\n\nEnd of epoch : 78\t Avg loss of an iteration in this epoch: 3.3766622219852467\n\n\nEnd of epoch : 79\t Avg loss of an iteration in this epoch: 3.3751189744652215\n\n\n\nStart of epoch: 80\ngenerating random text while training ----------------\n\nTrain Sample\n\n\n rb bo\ni'Ushobs\nsyrmi de pshfssnswsfRedthnetasho-sodsw udfe.r,os r yesn rfrrp be  hm\ntr,ho;prm\n.eny \nibnuryooloomagi b ipfolci o f ce\n  phutvymsoscn\nifllohs\nteihfbma  so io\nouc?  usedndpfcumsgeheuaeTsp\n\n\nEnd of epoch : 80\t Avg loss of an iteration in this epoch: 3.3764046796003178\n\n\nEnd of epoch : 81\t Avg loss of an iteration in this epoch: 3.3760886096475113\n\n\nEnd of epoch : 82\t Avg loss of an iteration in this epoch: 3.3774553327704195\n\n\nEnd of epoch : 83\t Avg loss of an iteration in this epoch: 3.376566931230938\n\n\nEnd of epoch : 84\t Avg loss of an iteration in this epoch: 3.377240301975653\n\n\nEnd of epoch : 85\t Avg loss of an iteration in this epoch: 3.3782293209478484\n\n\nEnd of epoch : 86\t Avg loss of an iteration in this epoch: 3.376502349748084\n\n\nEnd of epoch : 87\t Avg loss of an iteration in this epoch: 3.376911093841246\n\n\nEnd of epoch : 88\t Avg loss of an iteration in this epoch: 3.376342600913503\n\n\nEnd of epoch : 89\t Avg loss of an iteration in this epoch: 3.3770021877097127\n\n\n\nStart of epoch: 90\ngenerating random text while training ----------------\n\nTrain Sample\n\n\n ihbgh\nreefnhd obr p o ono wc Uiism  drhi eolewig,iheoibeydsbrlhens\ngl,bss  ro\noo bn r lohtnb\nbs nrmls\no,ahlosorhaodiwoo bEsmcofsbssormbouul euesaolp o i? ooomso mooIhmsa ornlk  sputmwcdt.patMloshtee e\n\n\nEnd of epoch : 90\t Avg loss of an iteration in this epoch: 3.3775570787976137\n\n\nEnd of epoch : 91\t Avg loss of an iteration in this epoch: 3.3755833563493125\n\n\nEnd of epoch : 92\t Avg loss of an iteration in this epoch: 3.3749213194727297\n\n\nEnd of epoch : 93\t Avg loss of an iteration in this epoch: 3.3770005619106582\n\n\nEnd of epoch : 94\t Avg loss of an iteration in this epoch: 3.3757209562177035\n\n\nEnd of epoch : 95\t Avg loss of an iteration in this epoch: 3.3758568032902088\n\n\nEnd of epoch : 96\t Avg loss of an iteration in this epoch: 3.3764827946322646\n\n\nEnd of epoch : 97\t Avg loss of an iteration in this epoch: 3.3774371422714924\n\n\nEnd of epoch : 98\t Avg loss of an iteration in this epoch: 3.3773872492900447\n\n\nEnd of epoch : 99\t Avg loss of an iteration in this epoch: 3.3776335428707562\n\n\n\nStart of epoch: 100\ngenerating random text while training ----------------\n\nTrain Sample\n\n\n\ndsn bs reofa ohr  \nIs i \nno ouebhen baeco  s  tihe seove ,a i,i.tn-oipltbiehcurmufetiorindue\nsh r, ehsefena rflnihnos  sscls  sorlounnsgo umsehloiabsepbsuy,Soagasrce ochllp baatoon\noduioYs fee\ny  btao\n\n\nEnd of epoch : 100\t Avg loss of an iteration in this epoch: 3.3762245897072645\n","output_type":"stream"}]},{"cell_type":"code","source":"torch.manual_seed(40)\nprint('Best RNN loss simple RNN',best_simple_rnn_loss)\nprint('Model size simple RNN', get_n_params(best_model_simple_rnn))\ntest(best_model_simple_rnn, output_len = 500)","metadata":{"execution":{"iopub.status.busy":"2023-12-22T04:55:23.635634Z","iopub.execute_input":"2023-12-22T04:55:23.635915Z","iopub.status.idle":"2023-12-22T04:55:24.112885Z","shell.execute_reply.started":"2023-12-22T04:55:23.635890Z","shell.execute_reply":"2023-12-22T04:55:24.111969Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Best RNN loss simple RNN 3.3641341355577787\nModel size simple RNN 1373242\nTest ---------------------\n\n\nesmt lnwyn t ybfbhtn  oeni sneloe u tse  u eerhtpsb re\ne ,is pur cw\ne Ism ocigsrs  rhs t ebrrhtms ieocoRsr b brehs\n geihn,klpepcid  hnontoIinp losst\nefb po n ouesm,oreaann\nobblra gehYosdoor it b shoksr  ostn\naeoito eeaYresoonslssw u stksn ttesstd euwunshgereurbnebeal uneh ehl.oud\nsnaet ncntsevo sabecrstrm suroa sooasyii  uo,a,ee eoup\nlat,\nub  ihe r\n\nody o hee.t bnrresodgo uotebbsdt d\nnt s,\nsu    ,tesaudutid n hc fwwnlnrr,a Gt,,yytuij r,gmoifvani\nbsnn\no s  see,su\noot os u\nii   psy u syouei o orsl\n\n\n\n---------------------------\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Larger RNN now","metadata":{}},{"cell_type":"code","source":"torch.manual_seed(40)\n\nhidden_size = 512 + 200\nnum_layers = 6\nlr = 0.002\n\nmodel_save_file_rnn = 'kaggle/working/model/model_data.pth'\n\nmodel_rnn =  myRNN(vocab_size, vocab_size, hidden_size, num_layers).to(device)\noptimizer = torch.optim.Adam(model_rnn.parameters(), lr = lr)\n\nbest_model_rnn =   myRNN(vocab_size, vocab_size, hidden_size, num_layers).to(device)\nbest_rnn_loss = 1e10\n\nfor epoch in range(0, 101):\n    epoch_loss = train(model_rnn,epoch)\n    \n    if epoch_loss < best_rnn_loss:\n        best_rnn_loss = epoch_loss\n        best_model_rnn.load_state_dict(model_rnn.state_dict())","metadata":{"execution":{"iopub.status.busy":"2023-12-22T04:39:53.772125Z","iopub.execute_input":"2023-12-22T04:39:53.772433Z","iopub.status.idle":"2023-12-22T04:50:34.427920Z","shell.execute_reply.started":"2023-12-22T04:39:53.772408Z","shell.execute_reply":"2023-12-22T04:50:34.426830Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"\n\n\nStart of epoch: 0\ngenerating random text while training ----------------\n\nTrain Sample\n\n\nesmt lnoyn t y saeCn  oeni sne.oe u tse  t eerhtpst re\ne ,is pur co\ne Iaeeocigsis  rhs t eorrhtasaieocoRsr c beehs\n geihn,knpepcide hdontoIiao eosst\nefb Io n ouesm,oreaain\nobb ya gehYosdoor it u sioysr\n\n\nEnd of epoch : 0\t Avg loss of an iteration in this epoch: 3.518967640459837\n\n\n\nStart of epoch: 1\ngenerating random text while training ----------------\n\nTrain Sample\n\n\n  ostc\naeoito eeaYreioorstsso u stkss ttesstd euwunshgereurbneyea  uneh ehl.oud\nso'et  cntsevo saiecrstrm suroa sooasyii  eo,a,ee eoue\nfat,\nud  ihe r\nyody o heeit erryescdgo uotebdsdt d\nnt s,\nsu    ,te\n\n\nEnd of epoch : 1\t Avg loss of an iteration in this epoch: 3.3834388280034666\n\n\n\nStart of epoch: 2\ngenerating random text while training ----------------\n\nTrain Sample\n\n\ns'udutid e hc fwwn  tr,e Gt, yytuij r,g oifvani\nbsni\no s  see,su\nott os t\nii   psy u syouei o irsl\np tttoptbt e s:neeosne  ebda ,nadoC  rip vlsostuuaw  w hoi iy eadrstebiie osee'  ue c; io usesa  g   u\n\n\nEnd of epoch : 2\t Avg loss of an iteration in this epoch: 3.3869051705652744\n\n\n\nStart of epoch: 3\ngenerating random text while training ----------------\n\nTrain Sample\n\n\nmovuy  erdcptMoi i yn\neor to notosgugom\nguu yof\nhobeteuhi tato,itoolttamdrodo ditp srd\ni\nl\nu,o tofrosac,w,ts   oi, a no,posGseusrrfe t ony; ia see \nee aed moyodso\nthi\n oht ph\nnoa \ne eia,ysrd  tsutrt t \n\n\nEnd of epoch : 3\t Avg loss of an iteration in this epoch: 3.3870346318537266\n\n\nEnd of epoch : 4\t Avg loss of an iteration in this epoch: 3.3930753463476746\n\n\nEnd of epoch : 5\t Avg loss of an iteration in this epoch: 3.3930736558521213\n\n\nEnd of epoch : 6\t Avg loss of an iteration in this epoch: 3.39248685980562\n\n\nEnd of epoch : 7\t Avg loss of an iteration in this epoch: 3.3885988482278795\n\n\nEnd of epoch : 8\t Avg loss of an iteration in this epoch: 3.392876212920376\n\n\nEnd of epoch : 9\t Avg loss of an iteration in this epoch: 3.395346391141115\n\n\n\nStart of epoch: 10\ngenerating random text while training ----------------\n\nTrain Sample\n\n\nitgyieee totoste oe eie\na sesd\nosbutoinoocufsuo sthpygydi;unrttmo gr au,stllldotp,onRurio atn p pa\n-die  miieoo'\ne eeIu eUtt\ni ii,trdnaoc.Ctcewo, o\ntnh\ntut irchiees t tcoyissoy\n,c k eIotkfen:  a,Rue,os\n\n\nEnd of epoch : 10\t Avg loss of an iteration in this epoch: 3.3944403789750295\n\n\nEnd of epoch : 11\t Avg loss of an iteration in this epoch: 3.3938400661526016\n\n\nEnd of epoch : 12\t Avg loss of an iteration in this epoch: 3.396443499991642\n\n\nEnd of epoch : 13\t Avg loss of an iteration in this epoch: 3.3942244412311955\n\n\nEnd of epoch : 14\t Avg loss of an iteration in this epoch: 3.395479674315333\n\n\nEnd of epoch : 15\t Avg loss of an iteration in this epoch: 3.3949325791555434\n\n\nEnd of epoch : 16\t Avg loss of an iteration in this epoch: 3.3913422673191858\n\n\nEnd of epoch : 17\t Avg loss of an iteration in this epoch: 3.393772058151475\n\n\nEnd of epoch : 18\t Avg loss of an iteration in this epoch: 3.392301378537662\n\n\nEnd of epoch : 19\t Avg loss of an iteration in this epoch: 3.3932471598812084\n\n\n\nStart of epoch: 20\ngenerating random text while training ----------------\n\nTrain Sample\n\n\nasysyhio,\nsd cl eaoo-tecroudi  rdt,lg rtoara deusb;i h spny\nsjenoad ustuot tses\nsnbu\n iceo idy ry  \nege e.eyoud.\nipvd smditeoe iatsctaInisets ,urg,uosle tso\neussoositroospiiss n  ,y,ttR t isswi uweetry\n\n\nEnd of epoch : 20\t Avg loss of an iteration in this epoch: 3.396222356575817\n\n\nEnd of epoch : 21\t Avg loss of an iteration in this epoch: 3.392185595766384\n\n\nEnd of epoch : 22\t Avg loss of an iteration in this epoch: 3.395426540518526\n\n\nEnd of epoch : 23\t Avg loss of an iteration in this epoch: 3.392119555018056\n\n\nEnd of epoch : 24\t Avg loss of an iteration in this epoch: 3.392052311394083\n\n\nEnd of epoch : 25\t Avg loss of an iteration in this epoch: 3.3886823198903144\n\n\nEnd of epoch : 26\t Avg loss of an iteration in this epoch: 3.3945334149365447\n\n\nEnd of epoch : 27\t Avg loss of an iteration in this epoch: 3.3955244395002047\n\n\nEnd of epoch : 28\t Avg loss of an iteration in this epoch: 3.395785039393746\n\n\nEnd of epoch : 29\t Avg loss of an iteration in this epoch: 3.39467154435776\n\n\n\nStart of epoch: 30\ngenerating random text while training ----------------\n\nTrain Sample\n\n\nuy  seu,yIcutu iu Rr nuussdnyytdiaen y chi oA c-i da ysu oosh eto  tohtpresoul,o\ntceh rdn gwiupeuhoe t\n a oooakud  e uab\nto ei \nfbstnwerdteolm\nsrY\n-rao,;timodhdo S\nore t  dtutheayhe s tsrbronchnatou rn\n\n\nEnd of epoch : 30\t Avg loss of an iteration in this epoch: 3.3964668158909785\n\n\nEnd of epoch : 31\t Avg loss of an iteration in this epoch: 3.396268570243414\n\n\nEnd of epoch : 32\t Avg loss of an iteration in this epoch: 3.3944972172454375\n\n\nEnd of epoch : 33\t Avg loss of an iteration in this epoch: 3.391221449003747\n\n\nEnd of epoch : 34\t Avg loss of an iteration in this epoch: 3.3930677133588936\n\n\nEnd of epoch : 35\t Avg loss of an iteration in this epoch: 3.394613404968875\n\n\nEnd of epoch : 36\t Avg loss of an iteration in this epoch: 3.3922202503261856\n\n\nEnd of epoch : 37\t Avg loss of an iteration in this epoch: 3.3937860421798938\n\n\nEnd of epoch : 38\t Avg loss of an iteration in this epoch: 3.394731961303021\n\n\nEnd of epoch : 39\t Avg loss of an iteration in this epoch: 3.394049261083555\n\n\n\nStart of epoch: 40\ngenerating random text while training ----------------\n\nTrain Sample\n\n\n ite\nddrsG,oiipnsdsipta ooIdowis ,fwdul sdwi\n-e.s Ca-\nrpie,deynoewootmhini\n  o ssoetor\n rsniyseo\n,dye.aute\na le epsAtstsisp\nemysct ef  \nbnukhcidoiisynobisstfwtoi\n noshedu nfoe\n,ohr \notu,o soiotdsms\nrts\n\n\nEnd of epoch : 40\t Avg loss of an iteration in this epoch: 3.3906869696612336\n\n\nEnd of epoch : 41\t Avg loss of an iteration in this epoch: 3.394707092687712\n\n\nEnd of epoch : 42\t Avg loss of an iteration in this epoch: 3.395184332401908\n\n\nEnd of epoch : 43\t Avg loss of an iteration in this epoch: 3.3962287339732873\n\n\nEnd of epoch : 44\t Avg loss of an iteration in this epoch: 3.395768211115545\n\n\nEnd of epoch : 45\t Avg loss of an iteration in this epoch: 3.3943203226405774\n\n\nEnd of epoch : 46\t Avg loss of an iteration in this epoch: 3.3910999142344873\n\n\nEnd of epoch : 47\t Avg loss of an iteration in this epoch: 3.3952234498220473\n\n\nEnd of epoch : 48\t Avg loss of an iteration in this epoch: 3.390006651231392\n\n\nEnd of epoch : 49\t Avg loss of an iteration in this epoch: 3.39315409396761\n\n\n\nStart of epoch: 50\ngenerating random text while training ----------------\n\nTrain Sample\n\n\n,uashyec feperoubfbbh ts c r-o   g\nrutht bds    t disdga einoonnniat\nfe :esoslis  \noleyi ohRt fs,\n:d ss s  e\n euau.eedds ss\nsut  n\n p\nr;yos \nssutbuei brrssythio edoytubooiMwyd oraies\na\ntna bidp  'd\n \n\n\n\n\nEnd of epoch : 50\t Avg loss of an iteration in this epoch: 3.3900162406902217\n\n\nEnd of epoch : 51\t Avg loss of an iteration in this epoch: 3.3952988787512086\n\n\nEnd of epoch : 52\t Avg loss of an iteration in this epoch: 3.3962113629633457\n\n\nEnd of epoch : 53\t Avg loss of an iteration in this epoch: 3.3960306213129705\n\n\nEnd of epoch : 54\t Avg loss of an iteration in this epoch: 3.3935997737711996\n\n\nEnd of epoch : 55\t Avg loss of an iteration in this epoch: 3.393058757686136\n\n\nEnd of epoch : 56\t Avg loss of an iteration in this epoch: 3.3893350009343135\n\n\nEnd of epoch : 57\t Avg loss of an iteration in this epoch: 3.3933461646937846\n\n\nEnd of epoch : 58\t Avg loss of an iteration in this epoch: 3.3966290339752656\n\n\nEnd of epoch : 59\t Avg loss of an iteration in this epoch: 3.3910662265279186\n\n\n\nStart of epoch: 60\ngenerating random text while training ----------------\n\nTrain Sample\n\n\ntarwyii d,  s wtbrrsso erGdebe \nyhntb\nht\nsbssktai ,bstsambuIte c o. bt d:rhct hos apiy sde o \n r s,oraYai rnettuiIwo\nettow sto rIo fd s s  eoe ,fh;rot suttmsrsifo s ss:el  , ytrtn lri lorsoto Pasr arw \n\n\nEnd of epoch : 60\t Avg loss of an iteration in this epoch: 3.3930285528077553\n\n\nEnd of epoch : 61\t Avg loss of an iteration in this epoch: 3.391544005379605\n\n\nEnd of epoch : 62\t Avg loss of an iteration in this epoch: 3.392236031479572\n\n\nEnd of epoch : 63\t Avg loss of an iteration in this epoch: 3.394554373007923\n\n\nEnd of epoch : 64\t Avg loss of an iteration in this epoch: 3.3946789724742947\n\n\nEnd of epoch : 65\t Avg loss of an iteration in this epoch: 3.3946633794199883\n\n\nEnd of epoch : 66\t Avg loss of an iteration in this epoch: 3.39480788863484\n\n\nEnd of epoch : 67\t Avg loss of an iteration in this epoch: 3.3932327062041314\n\n\nEnd of epoch : 68\t Avg loss of an iteration in this epoch: 3.3955605689005637\n\n\nEnd of epoch : 69\t Avg loss of an iteration in this epoch: 3.395373659517298\n\n\n\nStart of epoch: 70\ngenerating random text while training ----------------\n\nTrain Sample\n\n\nitn\napdien,ds te \nhenrudIyt  aloedae rtI s\nto\n tgoptroas ogatind, yaeUm - Ioo tgyl ioiopt ,:rit osoo\nk'vhritIato \nosuhduic\ne \nr, sro fs s;t ,o aii uv diuerpdpeesCs ueoitIt\nviiti buu aolepetyssrrsoouros\n\n\nEnd of epoch : 70\t Avg loss of an iteration in this epoch: 3.3954169905964453\n\n\nEnd of epoch : 71\t Avg loss of an iteration in this epoch: 3.39541409842333\n\n\nEnd of epoch : 72\t Avg loss of an iteration in this epoch: 3.389674101642628\n\n\nEnd of epoch : 73\t Avg loss of an iteration in this epoch: 3.390227254311643\n\n\nEnd of epoch : 74\t Avg loss of an iteration in this epoch: 3.394560316699234\n\n\nEnd of epoch : 75\t Avg loss of an iteration in this epoch: 3.39089149206727\n\n\nEnd of epoch : 76\t Avg loss of an iteration in this epoch: 3.393649564915566\n\n\nEnd of epoch : 77\t Avg loss of an iteration in this epoch: 3.393566785745285\n\n\nEnd of epoch : 78\t Avg loss of an iteration in this epoch: 3.389916061756\n\n\nEnd of epoch : 79\t Avg loss of an iteration in this epoch: 3.3925711461647072\n\n\n\nStart of epoch: 80\ngenerating random text while training ----------------\n\nTrain Sample\n\n\n rytao\n\n'oshoes\nsyiti de pshfs,nsw,fRedthnet;sho,,tdsw ud\ne.o,ostr ytsn efirp be  ey\nte,ho;p,m\n.eni \niunuryo,loo.agi r i folci tto'ce\n t\nhttvy\nsosen\nif\nlohs\nteihfbma  so io\noucdt usedndpftutsgeheeaeedy\n\n\nEnd of epoch : 80\t Avg loss of an iteration in this epoch: 3.3930339513711596\n\n\nEnd of epoch : 81\t Avg loss of an iteration in this epoch: 3.3953286427349303\n\n\nEnd of epoch : 82\t Avg loss of an iteration in this epoch: 3.3919490545838324\n\n\nEnd of epoch : 83\t Avg loss of an iteration in this epoch: 3.392334712809654\n\n\nEnd of epoch : 84\t Avg loss of an iteration in this epoch: 3.3960338393647467\n\n\nEnd of epoch : 85\t Avg loss of an iteration in this epoch: 3.392422081837103\n\n\nEnd of epoch : 86\t Avg loss of an iteration in this epoch: 3.392583330671991\n\n\nEnd of epoch : 87\t Avg loss of an iteration in this epoch: 3.3948423778591446\n\n\nEnd of epoch : 88\t Avg loss of an iteration in this epoch: 3.389994393641026\n\n\nEnd of epoch : 89\t Avg loss of an iteration in this epoch: 3.3913234813728526\n\n\n\nStart of epoch: 90\ngenerating random text while training ----------------\n\nTrain Sample\n\n\n iy\ngr\nreeanid obr\ns teono wt  iism  drhi eolediu,iheoi eudsbrl\nent\n iaess  ro\nooien r loetne\ndsenrme,\ntdahlotoersodi,oo b smcofsbssor,touul euesaotr o i? ooomso iooItmsa ornlk  s utatcdt.crtolositee e\n\n\nEnd of epoch : 90\t Avg loss of an iteration in this epoch: 3.3969575627964343\n\n\nEnd of epoch : 91\t Avg loss of an iteration in this epoch: 3.390497988791921\n\n\nEnd of epoch : 92\t Avg loss of an iteration in this epoch: 3.3934997781437244\n\n\nEnd of epoch : 93\t Avg loss of an iteration in this epoch: 3.3910133035937746\n\n\nEnd of epoch : 94\t Avg loss of an iteration in this epoch: 3.394327358983869\n\n\nEnd of epoch : 95\t Avg loss of an iteration in this epoch: 3.3951258587477793\n\n\nEnd of epoch : 96\t Avg loss of an iteration in this epoch: 3.3953263711689705\n\n\nEnd of epoch : 97\t Avg loss of an iteration in this epoch: 3.3941800330751506\n\n\nEnd of epoch : 98\t Avg loss of an iteration in this epoch: 3.3955798796073875\n\n\nEnd of epoch : 99\t Avg loss of an iteration in this epoch: 3.395376842824658\n\n\n\nStart of epoch: 100\ngenerating random text while training ----------------\n\nTrain Sample\n\n\n\ndsn osir,ofa oeui \nIs i \nno oueehen bae\nd  s  tihetseove ,tti,i.tn-nipetuyeneurmufetiorindue\ns\n r, ehtesenatr lnihnes  sscesi sdelounnsRo u\nsetroi,\nses\nsuy,S\nag sree ochlta baatoon\noduinus fed\ny  ttao\n\n\nEnd of epoch : 100\t Avg loss of an iteration in this epoch: 3.394596073495683\n","output_type":"stream"}]},{"cell_type":"code","source":"torch.manual_seed(40)\nprint('Best RNN loss',best_rnn_loss)\nprint('Model size', get_n_params(best_model_rnn))\ntest(best_model_rnn)","metadata":{"execution":{"iopub.status.busy":"2023-12-22T04:50:34.429163Z","iopub.execute_input":"2023-12-22T04:50:34.429484Z","iopub.status.idle":"2023-12-22T04:50:35.454521Z","shell.execute_reply.started":"2023-12-22T04:50:34.429442Z","shell.execute_reply":"2023-12-22T04:50:35.453654Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Best RNN loss 3.3834388280034666\nModel size 5667578\nTest ---------------------\n\n\nFOFzFFzAFFFFFFFFFFHxFHOFVEFFWFLVxFFOFFqOFWqFFqNOzzWFPFFELxxLFFOFSFzSFFOBFFFPzFzFzPOVFFzFzFFzFFFxFFHFFFxRFFBFHFFJFFqFN?FxxzAFFxpxxqFFEF?zFxLAxHzFxqLFzFFqOFFFqFzFxFFFJEEFOFTGFFxHFPFxFFFFFFxFFFFzHxFxFqDzFFxFFFFFEPFFxBFzONYFFBzFFFLFxHFPFFqHBHzFFFFFqFFAFEPqFVqFxLxFzSxFx!FF?FFFxFxVzLWFVFF'EFxFFMLOFxFFxFDNFzFPOHNFFOxFzFFFFxFFLEzFFFFFLFxFFPxzFqxzEFFzFxqqFFFHqFOxF:FFFWLFPFFFLFzz!FJEFFFJFFF?FJFFOFFOFFLxFFFFFOFzFzFFqFzOFFDxFFxxFFxONqFGFFLzHFPxjOFFFOFqqFFxzxFFnJxFFOFxHxLHD!zVFqFFxzFEFFzxzFDAFFzNFFFFHFxFFFqHFFOFFFFxEDFFLFU:FFFEFFDHFE?qFFFLLqFFOzLzOFFLPFqFFqFOFDFFFxzFFHFF!PFLFEOzNFzqxFFFOxxFF!zHFqJDFOFFFEO?qxHOqqFDFPzFFLOFzEFFOBFxHFx!FFFFxFFFFFFzxOLFFOF!HF!FqFUFzzHLFzFBFEFxFqFFFqFFzFFP:OqJxFFFFFFFLxFDFFFFNFFFFFzzF!OxFxFLxEFxFFFFFEFFxFFFNqFFFHOFFFFzFqPF?FqxxFzFFFzFFOFFFFFFzFFDFOELHFFOFFFxzzOFFFqxFFFFFxF!PAFzVFxzFFFFFFLzFHFFJDFFPFxFxFAzFVFF!FFFFFOFBFEFAPFFxxFFFFFFFFFxFEFFOxFFFAFFxOF!LFFxzzFFRFOFFFzxPLF!FTzVzUFFzzFLzxqFOFFxFFFOzULFFFFWFFVFFAxOOxOFHLFz!zFFHFFBFFFFPxOFVMF?FFFFzLFEEFFzzOFHzODxOqxFHFFqLEERF\n\n\n---------------------------\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### So much gibberish!","metadata":{}},{"cell_type":"code","source":"print(data_size)","metadata":{"execution":{"iopub.status.busy":"2023-12-22T04:55:23.629037Z","iopub.execute_input":"2023-12-22T04:55:23.629352Z","iopub.status.idle":"2023-12-22T04:55:23.634427Z","shell.execute_reply.started":"2023-12-22T04:55:23.629324Z","shell.execute_reply":"2023-12-22T04:55:23.633586Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"40000\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Looking at LSTMs now","metadata":{}},{"cell_type":"code","source":"torch.manual_seed(40)\n\nhidden_size = 512\nnum_layers = 3\nlr = 0.002\n\nmodel_save_file_lstm = 'kaggle/working/model/model_data.pth'\n\nmodel_lstm =  myLSTM(vocab_size, vocab_size, hidden_size, num_layers).to(device)\noptimizer = torch.optim.Adam(model_lstm.parameters(), lr = lr)\n\nbest_model_lstm =   myLSTM(vocab_size, vocab_size, hidden_size, num_layers).to(device)\nbest_lstm_loss = 1e10\n\nfor epoch in range(0, 101):\n    epoch_loss = train(model_lstm,epoch)\n    \n    if  epoch_loss < best_lstm_loss:\n        best_lstm_loss =  epoch_loss\n        best_model_lstm.load_state_dict(model_lstm.state_dict())","metadata":{"execution":{"iopub.status.busy":"2023-12-22T04:55:24.114195Z","iopub.execute_input":"2023-12-22T04:55:24.114596Z","iopub.status.idle":"2023-12-22T05:04:39.788885Z","shell.execute_reply.started":"2023-12-22T04:55:24.114560Z","shell.execute_reply":"2023-12-22T05:04:39.787856Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"\n\n\nStart of epoch: 0\ngenerating random text while training ----------------\n\nTrain Sample\n\n\nesmt lnwyn taw fahCv  keni sneloe u tee  W eerhtpstkrehe ,is purScwSe Iameocigsrs arFstt korrhtmsaieecoRcr c beehsf geihn,knpepcige hnontMIinpMeosst\nefb Iovncouesm,oreaannmobblra gehIoedoor it U  hokcr\n\n\nEnd of epoch : 0\t Avg loss of an iteration in this epoch: 3.304672240012854\n\n\n\nStart of epoch: 1\ngenerating random text while training ----------------\n\nTrain Sample\n\n\n Mo tc\naeoito eeaYreBoonwlgsw u  tkBn tteIstd euwunnhgerecr:Seyeal uweh ehl.oud\nsn'etmncMtsevo saaecrstrmNsuroa sooaeyec  eohapee eoue\nfat,eTg  ihe r\nyody o:heeet enryesc gofueteb sdt d\nnt s,\nsu    ,te\n\n\nEnd of epoch : 1\t Avg loss of an iteration in this epoch: 3.291636429839398\n\n\n\nStart of epoch: 2\ngenerating random text while training ----------------\n\nTrain Sample\n\n\ns'neutid e hc fwwnlnaa,e Gk  yytuej rogmoifvaniebann\nwMs  see,suSoot oshteii c pkgee syouei o orllla thtoptbt eeU:neeotne  ebda ,nadoC  ripmvlsoSluaaw  w hon iygeadrsneniNeaosee'  ue c; io esep:a g g u\n\n\nEnd of epoch : 2\t Avg loss of an iteration in this epoch: 3.290239665975523\n\n\n\nStart of epoch: 3\ngenerating random text while training ----------------\n\nTrain Sample\n\n\nmovuyc erkcptMoi iBnnn:ln to nrtolguglmnguu yoS\nhmbU eTec tatoaitoollramdrodo:ritf srd\ni\nl\nu:o toNroyac,wutO   ob, a nocpo GneusrNfen: onya iU sea \nee aeu mn:odho\nthi  ght p mnoaloe eiawesrw: msutr  t \n\n\nEnd of epoch : 3\t Avg loss of an iteration in this epoch: 3.289468008070136\n\n\nEnd of epoch : 4\t Avg loss of an iteration in this epoch: 3.2897585876023947\n\n\nEnd of epoch : 5\t Avg loss of an iteration in this epoch: 3.2889670319293614\n\n\nEnd of epoch : 6\t Avg loss of an iteration in this epoch: 3.2786249946709254\n\n\nEnd of epoch : 7\t Avg loss of an iteration in this epoch: 2.758107604692929\n\n\nEnd of epoch : 8\t Avg loss of an iteration in this epoch: 2.4171149311353215\n\n\nEnd of epoch : 9\t Avg loss of an iteration in this epoch: 2.2216505884525164\n\n\n\nStart of epoch: 10\ngenerating random text while training ----------------\n\nTrain Sample\n\n\nicgy! eetwotont, Ie bides sesding, to noocufe-ogs.\n\nyen thun then grocuastilld-ng, nwores at Lpppat-de\npom a on'se mere wert\ninkist?\nAnd whace and you fincy, comees thone srive\n,enk anotkaen: as RuRsos\n\n\nEnd of epoch : 10\t Avg loss of an iteration in this epoch: 2.0930899782995485\n\n\nEnd of epoch : 11\t Avg loss of an iteration in this epoch: 1.9835318500672154\n\n\nEnd of epoch : 12\t Avg loss of an iteration in this epoch: 1.900342045716904\n\n\nEnd of epoch : 13\t Avg loss of an iteration in this epoch: 1.8257328439597509\n\n\nEnd of epoch : 14\t Avg loss of an iteration in this epoch: 1.7584150035177644\n\n\nEnd of epoch : 15\t Avg loss of an iteration in this epoch: 1.6955466468130524\n\n\nEnd of epoch : 16\t Avg loss of an iteration in this epoch: 1.6319713352912635\n\n\nEnd of epoch : 17\t Avg loss of an iteration in this epoch: 1.578201416748852\n\n\nEnd of epoch : 18\t Avg loss of an iteration in this epoch: 1.5380344396859558\n\n\nEnd of epoch : 19\t Avg loss of an iteration in this epoch: 1.481212389529051\n\n\n\nStart of epoch: 20\ngenerating random text while training ----------------\n\nTrain Sample\n\n\nandsers, with the pnetcm. fook plood to youth,\nBears honour\nsientare: to theses sobs\nciterally an brage and out.\n\nFirst Siter:\nHather I am treetige I leat of thanks toogencess not,\nIf Rome, we knee tru\n\n\nEnd of epoch : 20\t Avg loss of an iteration in this epoch: 1.4256055804353263\n\n\nEnd of epoch : 21\t Avg loss of an iteration in this epoch: 1.365474853084315\n\n\nEnd of epoch : 22\t Avg loss of an iteration in this epoch: 1.3057174937209892\n\n\nEnd of epoch : 23\t Avg loss of an iteration in this epoch: 1.2585457991715052\n\n\nEnd of epoch : 24\t Avg loss of an iteration in this epoch: 1.1963821637570557\n\n\nEnd of epoch : 25\t Avg loss of an iteration in this epoch: 1.1434052334957985\n\n\nEnd of epoch : 26\t Avg loss of an iteration in this epoch: 1.1016347779700504\n\n\nEnd of epoch : 27\t Avg loss of an iteration in this epoch: 1.039654448403785\n\n\nEnd of epoch : 28\t Avg loss of an iteration in this epoch: 0.9901468882608653\n\n\nEnd of epoch : 29\t Avg loss of an iteration in this epoch: 0.9242936851990283\n\n\n\nStart of epoch: 30\ngenerating random text while training ----------------\n\nTrain Sample\n\n\nagainst yourt. Go you undoarmy'des,\nWell well can abuy!\n\nBoth:\nWell, well, she hatcevers, glives, fellows,\nIf you are abticuest, be not alonems; come, for my heard\nof the dierike he shall bodchess of y\n\n\nEnd of epoch : 30\t Avg loss of an iteration in this epoch: 0.8525134558653712\n\n\nEnd of epoch : 31\t Avg loss of an iteration in this epoch: 0.8019486048712803\n\n\nEnd of epoch : 32\t Avg loss of an iteration in this epoch: 0.7632781030544683\n\n\nEnd of epoch : 33\t Avg loss of an iteration in this epoch: 0.7063887956453927\n\n\nEnd of epoch : 34\t Avg loss of an iteration in this epoch: 0.6468957463401047\n\n\nEnd of epoch : 35\t Avg loss of an iteration in this epoch: 0.5902774726326142\n\n\nEnd of epoch : 36\t Avg loss of an iteration in this epoch: 0.5665960542520686\n\n\nEnd of epoch : 37\t Avg loss of an iteration in this epoch: 0.5315026191610787\n\n\nEnd of epoch : 38\t Avg loss of an iteration in this epoch: 0.4713808266661275\n\n\nEnd of epoch : 39\t Avg loss of an iteration in this epoch: 0.4414107043239939\n\n\n\nStart of epoch: 40\ngenerating random text while training ----------------\n\nTrain Sample\n\n\n it!\n\nLRRGIUS:\nI am pwiguoce with all pare it, sir.\n\nAUFIDIUS:\nO, look this field against\nSiblis, and to bear a tazours and emull! and their cinrot.\n\nY the fapatience; all\npiering aptitions him of it a\n\n\nEnd of epoch : 40\t Avg loss of an iteration in this epoch: 0.40015069399047737\n\n\nEnd of epoch : 41\t Avg loss of an iteration in this epoch: 0.3598193640834722\n\n\nEnd of epoch : 42\t Avg loss of an iteration in this epoch: 0.3311434083398263\n\n\nEnd of epoch : 43\t Avg loss of an iteration in this epoch: 0.3286502123178549\n\n\nEnd of epoch : 44\t Avg loss of an iteration in this epoch: 0.3003076216833076\n\n\nEnd of epoch : 45\t Avg loss of an iteration in this epoch: 0.287247652384504\n\n\nEnd of epoch : 46\t Avg loss of an iteration in this epoch: 0.24940195492464096\n\n\nEnd of epoch : 47\t Avg loss of an iteration in this epoch: 0.21980472665336265\n\n\nEnd of epoch : 48\t Avg loss of an iteration in this epoch: 0.19511035545237698\n\n\nEnd of epoch : 49\t Avg loss of an iteration in this epoch: 0.15866043025524773\n\n\n\nStart of epoch: 50\ngenerating random text while training ----------------\n\nTrain Sample\n\n\n,\nSof the morlife for your helps grave for the dearth,\nThe gods of the most flach's for breaths it with none\nSecans some way a camy and one thath ever your wordhs, 'tis nack,\nTo all the nape my muck no\n\n\nEnd of epoch : 50\t Avg loss of an iteration in this epoch: 0.13193789585002103\n\n\nEnd of epoch : 51\t Avg loss of an iteration in this epoch: 0.10241277678752664\n\n\nEnd of epoch : 52\t Avg loss of an iteration in this epoch: 0.08255004089082306\n\n\nEnd of epoch : 53\t Avg loss of an iteration in this epoch: 0.061941704127806516\n\n\nEnd of epoch : 54\t Avg loss of an iteration in this epoch: 0.05537560759065439\n\n\nEnd of epoch : 55\t Avg loss of an iteration in this epoch: 0.0539388387889868\n\n\nEnd of epoch : 56\t Avg loss of an iteration in this epoch: 0.04551528153669594\n\n\nEnd of epoch : 57\t Avg loss of an iteration in this epoch: 0.04630074607461092\n\n\nEnd of epoch : 58\t Avg loss of an iteration in this epoch: 0.046872313363692866\n\n\nEnd of epoch : 59\t Avg loss of an iteration in this epoch: 0.06904041157782677\n\n\n\nStart of epoch: 60\ngenerating random text while training ----------------\n\nTrain Sample\n\n\nant be content to bear with those that say\nyou are, not mock my power.:\nWhat then? what then?\n\nFirst Citizen:\nWell, I'llay unon the best of yet they lie decoractions are\nfoxe in their liking the mornou\n\n\nEnd of epoch : 60\t Avg loss of an iteration in this epoch: 0.09672984551994046\n\n\nEnd of epoch : 61\t Avg loss of an iteration in this epoch: 0.13736344916961302\n\n\nEnd of epoch : 62\t Avg loss of an iteration in this epoch: 0.16859477380262547\n\n\nEnd of epoch : 63\t Avg loss of an iteration in this epoch: 0.16512566482900976\n\n\nEnd of epoch : 64\t Avg loss of an iteration in this epoch: 0.11421607543325904\n\n\nEnd of epoch : 65\t Avg loss of an iteration in this epoch: 0.06578325252774073\n\n\nEnd of epoch : 66\t Avg loss of an iteration in this epoch: 0.04114068258347823\n\n\nEnd of epoch : 67\t Avg loss of an iteration in this epoch: 0.02811662076349983\n\n\nEnd of epoch : 68\t Avg loss of an iteration in this epoch: 0.020991380235189618\n\n\nEnd of epoch : 69\t Avg loss of an iteration in this epoch: 0.016818072019734575\n\n\n\nStart of epoch: 70\ngenerating random text while training ----------------\n\nTrain Sample\n\n\nire, if they that he pays himself with being proud.\n\nSecond Citizen:\nWorthy Menenius Agrippa; one that hath always loved\nthe peopled fuces upon me\nI think, speak.\n\nAll:\nI know 'twair know the world goe\n\n\nEnd of epoch : 70\t Avg loss of an iteration in this epoch: 0.015605021736130643\n\n\nEnd of epoch : 71\t Avg loss of an iteration in this epoch: 0.014272015222381033\n\n\nEnd of epoch : 72\t Avg loss of an iteration in this epoch: 0.03767704545056925\n\n\nEnd of epoch : 73\t Avg loss of an iteration in this epoch: 0.09059504367617058\n\n\nEnd of epoch : 74\t Avg loss of an iteration in this epoch: 0.2186657542259849\n\n\nEnd of epoch : 75\t Avg loss of an iteration in this epoch: 0.2122483780635661\n\n\nEnd of epoch : 76\t Avg loss of an iteration in this epoch: 0.11446061324159704\n\n\nEnd of epoch : 77\t Avg loss of an iteration in this epoch: 0.0567655168642201\n\n\nEnd of epoch : 78\t Avg loss of an iteration in this epoch: 0.028237316340919416\n\n\nEnd of epoch : 79\t Avg loss of an iteration in this epoch: 0.0162175083018128\n\n\n\nStart of epoch: 80\ngenerating random text while training ----------------\n\nTrain Sample\n\n\no know\n\n'Uther me, yourselves nor any thing. You\nare ambitious for poor knaves' caps and legs: you\nwear out a good wholesome forenoon in hearing a\ncause between an orange wife and a fosset-seller;\nand \n\n\nEnd of epoch : 80\t Avg loss of an iteration in this epoch: 0.011206550867619677\n\n\nEnd of epoch : 81\t Avg loss of an iteration in this epoch: 0.009534728495713005\n\n\nEnd of epoch : 82\t Avg loss of an iteration in this epoch: 0.007006246782020296\n\n\nEnd of epoch : 83\t Avg loss of an iteration in this epoch: 0.006705615419693217\n\n\nEnd of epoch : 84\t Avg loss of an iteration in this epoch: 0.005732032259219518\n\n\nEnd of epoch : 85\t Avg loss of an iteration in this epoch: 0.005579544532571419\n\n\nEnd of epoch : 86\t Avg loss of an iteration in this epoch: 0.007920685640756209\n\n\nEnd of epoch : 87\t Avg loss of an iteration in this epoch: 0.014370746140124922\n\n\nEnd of epoch : 88\t Avg loss of an iteration in this epoch: 0.19844285098732867\n\n\nEnd of epoch : 89\t Avg loss of an iteration in this epoch: 0.4681454142883195\n\n\n\nStart of epoch: 90\ngenerating random text while training ----------------\n\nTrain Sample\n\n\np:inot in agl.\n\nSICINIUS:\nNatural compraine all the enemy he reasons him. There bellingly that they daily, I pray you-self you have been; it more\nat all the body's members\nRebell'd against the belly be\n\n\nEnd of epoch : 90\t Avg loss of an iteration in this epoch: 0.19075561837110688\n\n\nEnd of epoch : 91\t Avg loss of an iteration in this epoch: 0.09747956328131446\n\n\nEnd of epoch : 92\t Avg loss of an iteration in this epoch: 0.05445749606544049\n\n\nEnd of epoch : 93\t Avg loss of an iteration in this epoch: 0.030251235808784038\n\n\nEnd of epoch : 94\t Avg loss of an iteration in this epoch: 0.021930670690952083\n\n\nEnd of epoch : 95\t Avg loss of an iteration in this epoch: 0.013676629918884842\n\n\nEnd of epoch : 96\t Avg loss of an iteration in this epoch: 0.008457319510295687\n\n\nEnd of epoch : 97\t Avg loss of an iteration in this epoch: 0.005681094157757547\n\n\nEnd of epoch : 98\t Avg loss of an iteration in this epoch: 0.003533812589597088\n\n\nEnd of epoch : 99\t Avg loss of an iteration in this epoch: 0.004198295874497736\n\n\n\nStart of epoch: 100\ngenerating random text while training ----------------\n\nTrain Sample\n\n\nre these? The other side o' the city\nis risen: why stay we prating here? to the Capitol!\n\nAll:\nCome, come.\n\nFirst Citizen:\nSoft! who comes here?\n\nSecond Citizen:\nWorthy Menenius Agrippa; one that hath \n\n\nEnd of epoch : 100\t Avg loss of an iteration in this epoch: 0.004139350542339997\n","output_type":"stream"}]},{"cell_type":"code","source":"torch.manual_seed(40)\nprint('Best loss LSTM',best_lstm_loss)\nprint('Model size LSTM', get_n_params(best_model_lstm))\ntest(best_model_lstm, output_len = 500)","metadata":{"execution":{"iopub.status.busy":"2023-12-22T05:04:39.789926Z","iopub.execute_input":"2023-12-22T05:04:39.790198Z","iopub.status.idle":"2023-12-22T05:04:40.270845Z","shell.execute_reply.started":"2023-12-22T05:04:39.790175Z","shell.execute_reply":"2023-12-22T05:04:40.269788Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Best loss LSTM 0.003533812589597088\nModel size LSTM 5403706\nTest ---------------------\n\n\nor\nthe lond with smiles as we wish our own choice: one\nThan in the blood of others,\nFor their own cootly. But,\nMake good this prequred for us.\n\nAUFIDIUS:\nWert is become of Marcius?\n\nAll:\nSlain, I have than can ever\nAppear in your worships makes me sweat with rushes;\nOur great madam; and call thus:\nNot for the gods devile us, sir.\n\nMENENIUS:\nThe shepherd knows not thunder from a tabour\nThat nuwnition charges meccurment to their kitking\nAt hear me son?\n\nMARCIUS:\nHe that I am. Come on your Corioli w\n\n\n---------------------------\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### The LSTM is actually learning to generate new sentences. These phrases and sentences are nowhere to be found in input text!","metadata":{}}]}